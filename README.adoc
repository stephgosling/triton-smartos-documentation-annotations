Notes on SmartOS and Triton Data Centre
=======================================

A collection of gotchas/links/workarounds/common pitfalls/howtos gleaned whilst
learning how to run Triton. This is not a tutorial and assumes a basic working
knowledge of SmartOS.

= Resources

== The canonical documentation

The SmartOS docs are somewhat better than the Triton docs, the latter suffering
a bit from being out-of-date/incomplete. SmartOS docs are open, Triton currently
https://smartdatacenter.topicbox.com/groups/sdc-discuss/T9e09dbcc6e7f8847-M8692907c9d10fb8a7ec3243e/source-for-docs-at-docs-tritondatacenter-com[
are not].

* https://docs.tritondatacenter.com/public-cloud[End User documentation (i.e.
MNX customer docs)] but somewhat applicable to those who self-host.
* https://docs.tritondatacenter.com/private-cloud[Operator documentation (i.e.
how to run the thing yourself)]
* https://docs.smartos.org[SmartOS official documentation]
* https://github.com/TritonDataCenter/rfd[Triton RFD repository]
* https://github.com/tritondatacenter/triton-cns/blob/master/docs/operator-guide.md[Triton CNS overview]
* https://illumos.org/hcl/[The Illumos Hardware Compatability List]

== Community links

* https://smartdatacenter.topicbox.com/groups/sdc-discuss/[Triton Data Center Topicbox mailing list]
* https://smartos.topicbox.com/groups/smartos-discuss/[SmartOS Topicbox mailing list]
* https://illumos.topicbox.com/groups/discuss/[Illumos discussion Topicbox mailing list]
* https://macktronics.com/tritoninst.html[This is a fabulously informative post]
that for me was instructional when initially prototyping Triton.
* https://www.davepacheco.net/blog/2024/illumos-physical-memory/[very deep dive
into illumos memory management]. Part two https://www.davepacheco.net/blog/2024/illumos-swap/[here].

= Working in Triton

== CloudAPI and the Triton command-line client

On-prem Triton has no portal, so for end users (not operators) https://github.com/TritonDataCenter/sdc-cloudapi[CloudAPI]
is the only way to interact with it. It isn't enabled by default but is trivial
to do so: `sdcadm postsetup cloudapi`

https://www.npmjs.com/package/triton[triton] is a nodejs command line utility
which is simple enough to install (`sudo npm install -g triton`) but the
configuration is a little odd with self-signed certificates requiring the `-i`
option and even though documented, `TRITON_TLS_INSECURE=1` and
`SDC_TLS_INSECURE=1` do not work.

The setup process  will walk through a profile setup requiring the CloudAPI URL,
Triton username and the fingerprint of the SSH key for that user.

TIP: users (including operators it seems) can only see their own instances so,
for example, the admin user can only see Triton components.

== How Triton/SmartOS Manages Virtual Machines

Triton runs VMs via `zones(7)`; these can be native, lx or HVM in nature.
Describing these is out of scope, however, they are managed in the same way via
command line tools in SmartOS, or API equivalents thereof in Triton. The
following three APIs are the ones pertinent to end-users:

=== Package API

A package is a collection of properties and limits/boundaries, when coupled with
an instance image, can constitute a Virtual Machine. These properties are things
like disk quotas or vCPU allocations, memory limits etc. The https://github.com/TritonDataCenter/sdc-papi/blob/master/docs/index.md[Package API]
(hereafter PAPI) is the resouce that manages packages for a DC and it is tightly
coupled with the Virtual Machine API. By-and-large packages are immutable which
is a consequence of their use in billing, similarly, packages cannot be deleted
once created for the same reason.

PAPI features aren't well exposed in the Admin UI nor is there a native SmartOS
command-line tool. Triton has `sdc-papi(1)` which is merely a `curl` helper.

Notable features/gotchas:

* quotas in packages being less than the image size will fail silently on
provisioning.
* the Admin UI only exposes a subset of the PAPI methods, which are rather rich:
PAPI supports traits, parent packages, OS and network filtering

=== Image API

The https://github.com/TritonDataCenter/sdc-imgapi/blob/master/docs/index.md[Image API]
(hereafter IMGAPI) manages images in a DC. Images can come from upstream or can
be created (see link:README.adoc#triton-cloud-images[image creation below]).
IMGAPI describes images and their requirements via image manifests.

IMGAPI can be used via the Admin UI for searching for images or importing from
upstream, but beyond that is read-only; image creation necessarily needs
command-line access to `imgadm(8)` and `zfs(8)`.

=== Virtual Machine API

The https://github.com/TritonDataCenter/sdc-vmapi/blob/master/docs/index.md[Virtual Machine API]
(hereafter VMAPI) manages Virtual Machines in Triton. Its methods are well
exposed in the Admin UI as well as with `vmadm(8)` and the cluster-wide
`sdc-vmadm`.

== Hardware Virtual Machines

SmartOS supports both KVM and bhyve as VMMs. Bhyve is considerably more
performant, supports flexible disk storage and instance resizing which makes it
now the defacto choice for provisioning hardware virtual machines.

=== Triton Cloud Images

MNX maintain a repository of images in the public cloud (images.smartos.org)
containing:

* native zone images for SmartOS itself
* lx images of various Linux distributiosn
* HVM images for Alma, Debian, Rocky, Ubuntu and Void Linux

If you wish to build your own the packer configuration, scripts etc.
https://github.com/TritonDataCenter/triton-cloud-images[can be found here].

==== Creating a disk image from an ISO

The SmartOS/Triton documentation is still around KVM rather than Bhyve. The
process is similar however the `vmadm` documentation is out of date: it
mentions methods and flags that _only_ work with KVM and Bhyve is different.
https://smartos.topicbox.com/groups/smartos-discuss/T1d477bd26c796cad-M6ca9c8317093ee17879656c3[
This is a good thread] that walks the user through the process including the
differing commands.

==== Creating a disk image _not_ from an ISO

Example VM image creation from an existing disk image (e.g. lift-and-shift of an
existing VM or an appliance-like image as described here. This is based on
https://docs.tritondatacenter.com/private-cloud/images/kvm[the official
documentation] with tweaks/fixes accommodating bhyve and a raw image.

. On the headnode create a working directory in `/var/tmp`:
+
----
[root@headnode (cabin) /var/tmp]# mkdir talos-1.10.6
[root@headnode (cabin) /var/tmp]# cd talos-1.10.6/

----
. Fetch and decompress the source image:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# wget https://github.com/siderolabs/talos/releases/download/v1.10.6/metal-amd64.raw.zst
[root@headnode (cabin) /var/tmp/talos-1.10.6]# zstd -d metal-amd64.raw.zst
metal-amd64.raw.zst : 1306525696 bytes
----

. Create an instance definition:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# cat 1.10.6.vmdef.json
{
  "name": "1.10.6",
  "brand": "bhyve",
  "vcpus": 1,
  "autoboot": false,
  "ram": 1024,
  "quota": 20,
  "bootrom": "/usr/share/bhyve/uefi-rom.bin",
  "disks": [
    {
      "boot": true,
      "model": "virtio",
      "size": 10240
    }
  ],
  "nics": [
    {
      "nic_tag": "external",
      "ip": "dhcp",
      "primary": "true",
      "model": "virtio"
    }
  ]
}
----

. Create an empty instance from the definition:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# vmadm create -f talos-1.10.6.vmdef.json
Successfully created VM 8ab900bd-f4a2-4f60-9ccd-e54e51df84b4
----

. Noting the zone ID, copy the disk image to the raw device of the empty
instance:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# dd if=metal-amd64.raw of=/dev/zvol/rdsk/zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4/disk0 bs=16M
140+1 records in
140+1 records out
2356150272 bytes (2.2 GiB) transferred in 51.718938 secs (43 MiB/sec)
----
+
TIP: at this point you might boot the instance to make additional changes e.g.
configuration management or installing guest tools etc. (see also cdrom
attachment).

. Retrieve the zvol uuid for the for the instance storage:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# vmadm get 8ab900bd-f4a2-4f60-9ccd-e54e51df84b4 |  json -a disks | json -a zfs_filesystem
zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4/disk0
----

. Create a snapshot of the zvol and copy it to a file (in this example the quota
needed to be extended to allow the snapshot creation):
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# zfs get quota zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4
NAME                                        PROPERTY  VALUE  SOURCE
zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4  quota     30.3G  local
[root@headnode (cabin) /var/tmp/talos-1.10.6]# zfs set quota=40G zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4
[root@headnode (cabin) /var/tmp/talos-1.10.6]# zfs snapshot zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4/disk0@dataset
[root@headnode (cabin) /var/tmp/talos-1.10.6]# zfs send zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4/disk0@dataset | gzip > talos-1.10.6.zvol.gz
[root@headnode (cabin) /var/tmp/talos-1.10.6]# ls -l talos-1.10.6.zvol.gz
-rw-r--r--   1 root     root     216766946 Aug 10 13:32 talos-1.10.6.zvol.gz
[root@headnode (cabin) /var/tmp/talos-1.10.6]#
----

. Create a IMGAPI manifest (called `talos-1.10.6.manifest.json` that describes
the desired image:
+
----
{
  "v": "2",
  "uuid": "<from the output of uuid>",
  "owner": "<from the output of sdc-ldap s 'login=admin' | grep ^uuid | cut -d' ' -f2>",
  "name": "talos-1.10.6",
  "description": "Talos Linux 1.10.6 (SDC v0.0.1)",
  "version": "0.0.1",
  "state": "active",
  "disabled": false,
  "public": true,
  "os": "linux",
  "type": "zvol",
  "files": [
    {
      "sha1": "<from the output of sum -x sha1 /var/tmp/talos-1.10.6/talos-1.10.6.zvol.gz | cut -d' ' -f1>"
      "size": <from the output of ls -l /var/tmp/talos-1.10.6/talos-1.10.6.zvol.gz | awk '{ print $5 }'>,
      "compression": "gzip"
    }
  ],
  "requirements": {
    "networks": [
      {
        "name": "net0",
        "description": "public"
      }
    ],
   "brand": "bhyve",
   "bootrom": "uefi"
  },
  "image_size": "<as specified in disks.size in talos-1.10.6.vmdef.json>",
  "disk_driver": "virtio",
  "nic_driver": "virtio",
  "cpu_type": "host"
}
----

. Finally, import the image:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# sdc-imgadm import -m /var/tmp/talos-1.10.6/talos-1.10.6.manifest.json -f /var/tmp/talos-1.10.6/talos-1.10.6.zvol.gz
Imported image 5842ee10-75ef-11f0-9a71-dbc0178393f0 (talos-1.10.6, 0.0.1, state=unactivated)
...75ef-11f0-9a71-dbc0178393f0 [=======================================================>] 100% 206.73MB
Added file "/var/tmp/talos-1.10.6/talos-1.10.6.zvol.gz" (compression "gzip") to image 5842ee10-75ef-11f0-9a71-dbc0178393f0
Activated image 5842ee10-75ef-11f0-9a71-dbc0178393f0
----

The image will now be available for use.

=== Running Bhyve instances

==== Console output from bhyve instances

In order to access to the console of a bhyve instance via VNC, the instance must
be running with a UEFI bootrom rather than legacy BIOS. For public images this
is already set so happens automatically. For self-built images one must enable
it in the image https://smartdatacenter.topicbox.com/groups/sdc-discuss/Td4b1c1bb557bae11/spring-2024-hvm-images[thus]:

`sdc-imgadm update <uuid> requirements.brand=bhyve requirements.bootrom=uefi`

For instances not started/managed by Triton, the UEFI bootrom can be enabled on
a stopped instance directly via `vmadm(8)` on the given compute node:

----
vmadm stop <uuid>
vmadm update <uuid> bootrom="/usr/share/bhyve/uefi-rom.bin"
vmadm start <uuid>
----

==== Resizing bhyve guest compute parameters

Resizing instances cannot be done via the Admin UI. There are however three ways
to change the CPU and memory limits for a virtual machine. All require the
instance to be in the stopped state.

. via `vmadm(8)`
. via the Triton VMAPI: `sdc sdc-vmapi /vms/<instance uuid>?action=update -d '{"billing_id":"<new package uuid>"}'`
. via the https://smartdatacenter.topicbox.com/groups/sdc-discuss/T0609521b5cfbff31-Mc983b2dc841bdb365667bf46/resize-a-bhyve-instance[triton CLI]

==== Bhyve guest storage

By default, packages https://docs.tritondatacenter.com/private-cloud/install/advanced-configuration#install-sample-packages[imported in cluster setup]
or created via the Admin UI do not specify a disk layout. This means that any
package with a quota larger than the image size will have the additional disk space
presented to the guest as a second disk e.g. for a 30GB quota and 10GB image,
`disk0` (`/dev/vda`) of 10GB, `disk1` (`/dev/vdb`) of the remaining 20GB quota.

When specifying a disk layout in the Admin UI it is not possible to use the
https://github.com/TritonDataCenter/sdc-papi/blob/master/docs/index.md#default-disks-in-package[`remaining`]
parameter. This means when specifying a single disk of a given size, the
instance will need to be stopped and the disk resized (see below). While this is
slightly inconvenient it as the advantage of leaving space available for
snapshots.

To get EC2-like behaviour from a package, one must interact with PAPI directly
for example:

----
[root@headnode (cabin) ~]#  sdc-papi  /packages -X POST -d "$(cat test.json)"
HTTP/1.1 201 Created
request-id: 5ea63860-e95e-11f0-b700-07c5233659b9
request-id: 5ea63860-e95e-11f0-b700-07c5233659b9
Location: /packages/5ea63860-e95e-11f0-b700-07c5233659b9
Content-Type: application/json
Content-Length: 496
Access-Control-Allow-Origin: *
Access-Control-Allow-Headers: Accept, Accept-Version, Content-Length, Content-MD5, Content-Type, Date, Api-Version, Response-Time
Access-Control-Allow-Methods: GET, HEAD, POST
Access-Control-Expose-Headers: Api-Version, Request-Id, Response-Time
Connection: Keep-Alive
Content-MD5: 7Fz3t2T+UCJWh2CKKA4eeA==
Date: Sun, 04 Jan 2026 11:18:54 GMT
Server: SDC Package API 7.2.5
Api-Version: 7.2.5
Response-Time: 201

{
  "brand": "bhyve",
  "uuid": "5ea63860-e95e-11f0-b700-07c5233659b9",
  "name": "xxtestflex20260104b",
  "version": "0.0.1",
  "owner_uuids": [
    "9cbc0ae6-d9b6-4185-9459-238e0bc466f4"
  ],
  "active": true,
  "vcpus": 1,
  "cpu_cap": 100,
  "description": "",
  "max_lwps": 4000,
  "max_physical_memory": 2048,
  "max_swap": 4096,
  "quota": 20480,
  "zfs_io_priority": 64,
  "created_at": "2026-01-04T09:57:39.453Z",
  "updated_at": "2026-01-04T11:18:54.412Z",
  "billing_tag": "",
  "flexible_disk": true,
  "disks": [
    {
      "size": "remaining"
    }
  ],
  "default": false,
  "group": "",
  "v": 1
}
----

This package will assign a single disk of `quota` size and cloud-init will
automatically expand to fill the disk on boot.

==== Bhyve guest storage resizing

Although the Admin UI supports storage resizing, as with resizing instances, it
lags behind the triton CLI. To resize a boot or data disk this can be done on
the given compute node. In the examples below `X` is the linux device, `Y` is
the corresponding ZFS zvol, so `/dev/vda` mapping to `disk0`, `/dev/vdb` to
`disk1` etc.

===== Downsizing

. in the guest unmount the volume and `resize2fs /dev/vdX somesizeM` to shrink
the filesystem.
. shut the instance down
. set the new zvol size on the host compute node: `zfs set volsize=somesize+bufferM zones/<uuid>/diskY`
`somesize+buffer` simply for safety
. boot the instance again and `resize2fs /dev/vdX` to grow the filesystem to the
end of the device

===== Upsizing

. shut the instance down
. set the new zvol size on the host compute node: `zfs set volsize=somesizeM zones/<uuid>/diskY`
. boot the instance
.. if the zvol in question is the boot volume then `cloud-init` will
automatically resize the partition (typically `/dev/vda4`) and extend the
filesystem.
.. if the zvol is a secondary device then depending on the layout `growpart` may
be needed in addition to `resize2fs`

=== Locality

Triton has good support for 3 different kinds of locality when provisioning
VMs, all https://docs.tritondatacenter.com/private-cloud/traits[well documented
here].

* Traits to help VMAPI schedule VMs based on tags added to images, packages and
CNs
* Locality rules (e.g. _near_ and _far_) to locate workloads appropriately
* Rack-awareness

Notes/gotchas:

* provisioning will fail if traits can't be satisified

= Managing Triton

== Triton component dependencies

Triton is made up of a variety of services offering APIs exposing functionality.
They are all described https://docs.tritondatacenter.com/private-cloud/resilience[
in the HA documentation] but here is a non-exhaustive list of dependencies I've
discovered in debugging headnode bringup on slow storage:

* lots of things depend on https://docs.tritondatacenter.com/private-cloud/troubleshooting/manatee[Manatee],
such as Moray (the cluster kv store)
* UFDS (the clusters directory server) depends on moray
* amon (the clusters monitoring and alerting service) depend on CNAPI, redis,
VMAPI, and UFDS
* FWAPI (the firewall API) depends on UFDS and Moray
* VMAPI (described link:README.adoc#virtual-machine-api[here]) depends on the
workflow API, CNAPI, IMGAPI, NAPI, PAPI, VOLAPI and moray

== General storage notes

* delegated datasets allow some ZFS administrative operations to happen in a
zone. Relevant really only for containers, not HVM.
* quotas can only be set in the global zone
* resizing a zone is as simple (in the global zone) as `zfs set quota=1T zones/9b469b7c-2b46-451b-bcc7-69de7d2f9a1f`
* recordsize can be set in a zone and only takes effect with new files: `zfs set recordsize=1M zones/9b469b7c-2b46-451b-bcc7-69de7d2f9a1f/data`

== Network storage services

=== NFS

This is a mix of inexperience wth v4 on my part, the https://docs.smartos.org/configuring-nfs-in-smartos/[
official SmartOS docs being _very_ out of date] (basically, do not use!) and
documentation being increasingly hard to find for things that are not Linux.

You _can_ enable NFS (or SMB) in the global zone but that feels icky if you
think about unix users and other changes needed. Instead I opted to run multiple
zones (small ones too, only 512MB RAM) as that gives much more flexibility:

* changes persist in the zones
* recordsize can be set per share as appropriate for the workload
* NFS share restrictions can also be set as appropriate

General pointers regarding NFS:

* Enabling NFS in a zone (presumably also the global zone, untested...) requires
`svcadm enable rpc/bind` which isn't on by default in Triton SmartOS.
* Enabling a share can be done in a (delegated dataset?) zone thus: `zfs set sharenfs='rw=@172.24.0.254/32' zones/9b469b7c-2b46-451b-bcc7-69de7d2f9a1f/data`
* NFSv4 needs DNS domains to match, not just Unix UIDs: `sharectl set -p nfsmapid_domain=chuci.org nfs`.
Without that, NFS from Linux will work but the mappings will be incorrect:
`user:group` are set correctly server-side but are `nobody:nogroup` client-side.
* In Ubuntu 24.04 (but _not_ 22.04) this also needed to be matched with an
equivalent domain in `/etc/idmapd.conf`.
* NFS performance on spinning disk is particularly poor as NFS will mount `rw`
filesystems `sync` for durability. This can be mitigated with a log special
vdev for the ZIL. See link:README.adoc#adding-a-mirrored-SLOG-to-a-pool[Adding a mirrored
SLOG to a pool]

=== SMB

This is an absolutely massive topic and the documentation is confusing and/or
out of date. My usecase was simple: migration of data from an equivalent share
that had resided on Ubuntu/Samba and had also been simultaneously shared
as NFS described above. No AD, and not even remotely complicated. What follows
is a hodgepodge of https://wiki.smartos.org/configuring-smb-in-smartos/[
the official documentation] (outdated) and https://illumos.topicbox.com/groups/developer/T853ccac866b92198-M029acf623527b9ff13bd3ada[this thread]. I didn't manage to get `sharemgr` to do anything useful
but was able to muddle through with `zfs set sharesmb` once the prerequisites
were done. So steps taken in a zone that was already sharing via NFS (i.e.
unix users matched what existed before/on other instances):

. Edit `pam.conf` to add `pam_smb_passwd.so.1` (see links above, note tabs)
. Enable the services:
+
----
svcadm enable smb/server
svcadm enable smb/client
svcadm enable rpc/bind # already enabled but included for completeness
svcadm enable idmap    # already enabled but included for completeness
----

. Create the unix group
. Create the unix users
. Enable the unix users as SMB users (needed so that their passwords can be
synchronized with `/var/smb/smbpasswd`
+
----
smbadm enable-user <username>
----

. Set the password for the unix user as normal (i.e `passwd <username>`)
. At this point everything wanted to do things with `sharemgr` but I couldn't
get them to work, however `sharesmb` (analogous to `sharenfs`) _did_ work for me:
+
----
zfs set sharesmb=on zones/9b469b7c-2b46-451b-bcc7-69de7d2f9a1f/data
----

=== Migrations

* migrations happen over the admin network so are limited by its speed.

=== CPU caps

By default CPU cap enforcement is https://github.com/TritonDataCenter/sdc-papi/blob/master/docs/index.md#sapi-configuration[enabled]
 and in any production or multi-tenant environment the recommendation is to
leave it on. This is to prevent perceived scheduling issues for different
tenants. Capped and non-capped workloads should never be mixed as this can cause
difficulties for the scheduling of VMAPI/CNAPI. If both kinds of workloads need
to exist a mitigation is with the use of https://docs.tritondatacenter.com/private-cloud/traits[traits].
This https://smartdatacenter.topicbox.com/groups/sdc-discuss/Tdee50d0ae7379e1d[conversation on the rationale, history and issues]
is very useful.

=== Fixing provisioning errors around "no compute resources"

A single provisioning error can cascade into a DC-wide problem. In one instance
I had a VM migration that went wrong and left a deleted VM still existing in
VMAPI even though it was long gone from its host compute node. The VM object
looked like this:
----
{
  "uuid": "4fe6dceb-37a4-4e18-983c-2230d1e4b802",
  "alias": null,
  "autoboot": null,
  "brand": null,
  "billing_id": null,
  "cpu_cap": null,
  "cpu_shares": null,
  "create_timestamp": null,
  "customer_metadata": {},
  "datasets": [],
  "destroyed": null,
  "firewall_enabled": false,
  "internal_metadata": {},
  "last_modified": null,
  "limit_priv": null,
  "max_locked_memory": null,
  "max_lwps": null,
  "max_physical_memory": null,
  "max_swap": null,
  "nics": [],
  "owner_uuid": null,
  "platform_buildstamp": null,
  "quota": null,
  "ram": null,
  "resolvers": null,
  "server_uuid": "9bb8490c-8aa8-1a29-a45c-d8bbc1cd9188",
  "snapshots": [],
  "state": null,
  "tags": {},
  "zfs_filesystem": null,
  "zfs_io_priority": null,
  "zone_state": null,
  "zonepath": null,
  "zpool": null,
  "image_uuid": null
}
----
amongst other things `cpu_cap: null` stopped CNAPI/VMAPI from automatically
choosing that compute node for new VM replacement, even for a miniscule 64MB
Joyent branded zone, because it would mean mixing capped and uncapped workloads.
The fix was to modify the object and setting the CPU cap via `sdc-vmapi`
 https://github.com/TritonDataCenter/sdc-vmapi/blob/master/docs/index.md#putvm-put-vmsuuid[as per the VMAPI documentation]:

----
sdc-vmapi /vms/4fe6dceb-37a4-4e18-983c-2230d1e4b802? -X PUT -d '{"cpu_cap": "100"}'
HTTP/1.1 200 OK
Connection: close
Content-Type: application/json
Content-Length: 73
Date: Fri, 03 Jan 2025 15:53:15 GMT
Server: VMAPI/9.16.0
x-request-id: 22cb050e-47f7-4bf8-a789-a7abc4810ca6
x-response-time: 129
x-server-name: 54406d2e-1c7c-45fc-a161-e5083e6a2d58

{
  "uuid": "4fe6dceb-37a4-4e18-983c-2230d1e4b802",
  "cpu_cap": "100",
  "tags": {}
}
----

With the cap back in place automatic allocation of new workloads was unblocked.

=== Protecting virtual machines

The Admin UI offers no guardrails for deleting HVMs (nor indeed compute nodes)
but their datasets can be protected with: `vmadm update <uuid> indestructible_zoneroot=true`
which will at least stop the zone (therefore, the data) being destroyed.


== Networking

Triton requires at a minimum of two networks to function in a networked
environment: the _admin_ network (completely segregated) and the _external_
network to interface with the existing network/the Internet. A third network
_underlay_ is needed for additional networking within the cluster as well as
more advanced services such as CNS and docker.

=== Guest agent

Instances that do not have the guest agent cannot be assigned IP addresses
from the Triton DHCP server. In order to get an assignment from an external DHCP
server _Allow DHCP Spoofing_ must be enabled in the Admin UI which corresponds
to the `nics["whichever"].allow_dhcp_spoofing` boolean in VMAPI. Unfortunately
this is wasteful in the `external` network address space as an address will be
assigned there but not used.

==== MTU, NICTags and networks

* As stated in the installation documentation, the underlay network MTU *must*
be 9000
* MTU for other networks can only be set at network creation time. This is true
even for the `admin` network which means it cannot be altered from 1500 as it is
not a tunable exposed in the installer. It might be possible to remedy by
creating a new network with the desired MTU and then changing the tags on the
NICs.

=== Compute Node networking

Compute nodes cannot use RealTek Gigabit Ethernet adapters for their admin nic.
For some reason (age?) `dladm` is unable to set the MTU on this driver even
though `show-linkprop rge0` said that the MTU property was read-write. This had
the side effect of a cascading failure for that node resulting in very odd
behaviour from `vmadm`. https://smartdatacenter.topicbox.com/groups/sdc-discuss/Td611bcbb977e00d1[
This is the Triton Topicbox thread].

== Hardware

=== Temperatures

Temperaturs are exposed via the FMA and can be read with `fmtopo` thus (8 core,
16 thread CPU so 16 temperatures):

----
[root@cn0 (cabin) ~]# /usr/lib/fm/fmd/fmtopo -V *sensor=temp | grep reading
    reading           double    37.625000
    reading           double    37.625000
    reading           double    37.625000
    reading           double    37.625000
    reading           double    37.625000
    reading           double    37.625000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
----

=== Disks

==== Disk usage patterns

On a clean install, the headnode uses around 60GB of actual storage but when
accounting for reservations/refreservations this figure is an overcommit of
around 1TB. The headnode may have issues when booting from rotating media: read
latency can cause services in zones to fail because they have timed out waiting
for a dependency to start.

Compute nodes can operate with hard drives but are not suited to synchronous
writes unless they have their ZIL moved to a log device (see
link:README.adoc#nfs[the NFS section]).

==== Reporting and Failures (SAS/SATA)

The FMA is S.M.A.R.T aware but doesn't expose the complete suite of reporting
that `smartmontools` does, which is not unexpected given that the reporting for
drives from different manufactureres all differ. `smartmontools` is available to
the headnode via `pkgsrc` but compute nodes can't install packages. `smartmontools`
may yet make it into the default SmartOS image but in event that this doesn't
happen, I should see how the PXE images are built, or, as a worst-case
`sdc-oneachnode` can drop a binary. My preferred approach here now is to drop an
agent container on each compute node with `/dev/rdsk/c*` exposed and
`smartmontools` installed.

==== Reporting and Failures (NVME)

`nvmeadm` can report various device statistics. First to enumerate drives:

----
[root@cn1 (cabin) ~]# nvmeadm list
nvme0: model: CT1000P1SSD8, serial: 1938E21FFB39, FW rev: P3CR013, NVMe v1.3
  nvme0/1 (c1t00A07519E21FFB39d0): Size = 931.51 GB, Capacity = 931.51 GB, Used = 931.51 GB
nvme1: model: PC401 NVMe SK hynix 1TB, serial: EJ7CN70461CA41T41, FW rev: 80003E00, NVMe v1.2
  nvme1/1 (c2tACE42E717000134Cd0): Size = 953.87 GB, Capacity = 953.87 GB, Used = 874.83 GB
[root@cn1 (cabin) ~]# zpool status
----

To report health:

----
[root@cn1 (cabin) ~]# nvmeadm get-logpage  nvme0 health
nvme0: SMART/Health Information
  Critical Warnings
    Available Space:                        OK
    Temperature:                            OK
    Device Reliability:                     OK
    Media:                                  OK
    Volatile Memory Backup:                 OK
  Temperature:                              61C
  Available Spare Capacity:                 100%
  Device Life Used:                         32%
----

==== recordsize, volblocksize, compression and write amplification

By default, the `zones` zpool recordsize is 128k compression is off. For HVM
workloads, when VMAPI creates a virtual machine the volblocksize is by default
8k for the disks and is not configurable currently. Received wisdom is that
enabling `lz4` (`zstd` is not yet available in Illumos ZFS) but this should be
done at CN setup time and my (current) understanding is that HVM instances would
need to be recreated as, in a zone migration, the zvols properties are going to
be retained. To enable compression:

----
[root@headnode (cabin) ~]# zfs get compression zones
NAME   PROPERTY     VALUE     SOURCE
zones  compression  off       default
[root@headnode (cabin) ~]# zfs set compression=lz4 zones
[root@headnode (cabin) ~]# zfs get compression zones
NAME   PROPERTY     VALUE     SOURCE
zones  compression  lz4       local
----

==== zpool disk selection

https://github.com/TritonDataCenter/smartos-live/blob/master/src/node_modules/disklayout.js[
This is the logic] that the installer uses when provisioning a new node. Broadly
it groups drives by media type and then size but doesn't always make desirable
choices e.g.

* for a host with both SSD and NVME it will choose the latter as log device(s)
* for a host with HDDs and more than one solid-state drive it will use the
latter as individual log devices rather than a mirror.

For more involved layouts you may want to start with base arrangement at install
time, then add drives once the node has been provisioned.

==== Adding a mirrored SLOG to a pool

This is simple to do.

CAUTION: Note once a log device has been added to a pool it *cannot* be removed,
only replaced.

----
[root@headnode (cabin) ~]# zpool status
  pool: zones
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        zones       ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            c2t2d0  ONLINE       0     0     0
            c2t3d0  ONLINE       0     0     0

errors: No known data errors
[root@headnode (cabin) ~]# diskinfo
TYPE    DISK                    VID      PID              SIZE          RMV SSD
SATA    c2t0d0                  Lexar    SSD NQ100 256GB   238.47 GiB   no  yes
SATA    c2t1d0                  Lexar    SSD NQ100 256GB   238.47 GiB   no  yes
SATA    c2t2d0                  TOSHIBA  MG09ACA14TE      13039.00 GiB  no  no
SATA    c2t3d0                  TOSHIBA  MG09ACA14TE      13039.00 GiB  no  no
[root@headnode (cabin) ~]# zpool add zones log mirror c2t0d0 c2t1d0
[root@headnode (cabin) ~]# zpool status
  pool: zones
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        zones       ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            c2t2d0  ONLINE       0     0     0
            c2t3d0  ONLINE       0     0     0
        logs
          mirror-1  ONLINE       0     0     0
            c2t0d0  ONLINE       0     0     0
            c2t1d0  ONLINE       0     0     0

errors: No known data errors
----

=== Booting from a ZFS pool

The headnode can be configured to boot from a ZFS pool rather than USB. This
need not be the `zones` pool but can be. Community members report that a
typical use case for booting from a separate pool is to side-step buggy firmware
where given drives on a controller can't be booted from. To enable:

----
[root@headnode (cabin) /zones]# piadm bootable -e zones
NOTE:  Directory zones/boot on pool zones is now able to be
this head node's virtual bootable USB key.

If this isn't a replacement pool for an existing bootable
pool, you should remove the USB key from this headnode
and then reboot this headnode from a disk in zones

The USB key (even if it's another pool's bootfs) is
now unmounted because that is what it was
before piadm ran.

[root@headnode (cabin) /zones]# piadm list
PI STAMP               BOOTABLE FILESYSTEM            BOOT IMAGE NOW  NEXT
20260122T000642Z       zones/boot                     next       yes  yes
----

=== Migrating or recovering physical nodes

* node UUID is based on https://smartdatacenter.topicbox.com/groups/sdc-discuss/Td611bcbb977e00d1-Mb9105949ffe023f1a0fe82d1/reprovisioning-a-cn-network-early-admin-service-failure[
on the serial number]. No matter how many reinstalls the a node with a given
motherboard will get the same UUID.
* steps for https://docs.tritondatacenter.com/private-cloud/troubleshooting/compute-node[
recovering a Compute Node are relatively straight forward].
* steps for gracefully migrating a headnode, albeit old, https://github.com/TritonDataCenter/triton/pull/172[
are in this pull request].
* steps for recovering a headnode involve physically moving the headnode disks,
as many NICs as is feasible and USB stick. SmartOS will start but Triton likely
will not because NIC labels (especially `admin`) may be wrong; this can be
corrected by mounting the USB stick (`sdc-usbkey mount`) and editing the NIC
tags in `/mnt/usbkey/config`. Following a reboot Triton should start but NICs in
NAPI will still need to be cleaned up, particularly if reusing headnode
hardware.
** `sdc-napi /nics | less` to list
** `curl -X PUT napi.{sdc domain}/nics/{mac} -d belongs_to_uuid=1002ff03-e004-0105-8e06-ad0700080009`
where the ownership is wrong (e.g. old headnode)
** `curl -X DELETE napi.{sdc domain}/nics/{mac}` where the NIC no longer exists

=== Debugging compute node installation issues

Once a CN has been successfully booted, even if it hasn't been adopted it can
still be interrogated with `sdc-oneachnode`.

== Miscellanea

=== ToDo

* headnode console session history
* CNS
* Certificates (needs CNS) (https://github.com/TritonDataCenter/triton-dehydrated[see also this]
* configuration management for compute nodes
* smartmontools for compute nodes
* quotas, reservations and refreservations

= Installing the headnode

The https://docs.tritondatacenter.com/private-cloud/install/headnode-installation[documentation]
is complete but some points and steps that are worth re-iterating:

* make absolutely sure that there is no semblance of anything ZFS related on
disks, it _will_ trip the installer up. Even recreating the partition table in
Linux is not guaranteed to clear everything. What _does_ work is to boot the
installer or a SmartOS/some other Illumos thumbdrive:
** `zpool import -a` to see if there are any pools there then `zpool destroy`
them
** then run `format` and in that `label` each of the disks.
* how long the installation will take depends on the speed of the install media.
A USB 3.0 stick takes approximately 10 minutes to copy the files.
* Once installed I performed the following:
** `piadm bootable -e zones` to enable pool booting, and reboot (see link:README.adoc#booting-from-a-zfs-pool[this])
** `zfs set compression=lz4 zones` to enable LZ4
