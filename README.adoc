Triton and SmartOS documentation annotations
============================================

A collection of gotchas/links/workarounds/common pitfalls/howtos gleaned whilst
learning how to run Triton and migrate VMs to it. This is absolutely not a
tutorial and assumes a basic working knowledge of Triton SDC and SmartOS.
Also started life as a testbed for learning AsciiDoc works.

= The canonical documentation

The SmartOS docs are somewhat better than the Triton docs, the latter suffering
a bit from being out-of-date/incomplete. SmartOS docs are open, Triton currently
https://smartdatacenter.topicbox.com/groups/sdc-discuss/T9e09dbcc6e7f8847-M8692907c9d10fb8a7ec3243e/source-for-docs-at-docs-tritondatacenter-com[
are not].

* https://docs.tritondatacenter.com/public-cloud[End User documentation (i.e.
MNX customer docs)] but somewhat applicable to those who self-host.
* https://docs.tritondatacenter.com/private-cloud[Operator documentation (i.e.
how to run the thing yourself)]
* https://docs.smartos.org[SmartOS official documentation]
* https://github.com/TritonDataCenter/rfd[Triton RFD repository]
* https://github.com/tritondatacenter/triton-cns/blob/master/docs/operator-guide.md[Triton CNS overview]

= Other useful resources

* https://smartdatacenter.topicbox.com/groups/sdc-discuss/[Triton Data Center Topicbox mailing list]
* https://smartos.topicbox.com/groups/smartos-discuss/[SmartOS Topicbox mailing list]
* https://illumos.topicbox.com/groups/discuss/[Illumos discussion Topicbox mailing list]
* https://macktronics.com/tritoninst.html[This is a fabulously informative post]
that for me was instructional when initially prototyping Triton.
* https://www.davepacheco.net/blog/2024/illumos-physical-memory/[very deep dive
into illumos memory management]. Part two https://www.davepacheco.net/blog/2024/illumos-swap/[here].

= Hardware Virtual Machines

SmartOS supports both KVM and bhyve as VMMs. Only bhyve supports flexible disk
storage and instance resizing, making it the preferred choice.

== Building Bhyve images

=== Triton Cloud Images

MNX maintain Triton images of Alma, Debian, Rocky, Ubuntu and Void, and these
can be imported from the public cloud (images.smartos.org) directly in the
console. If you wish to build your own the packer configuration, scripts etc.
https://github.com/TritonDataCenter/triton-cloud-images[can be found here].

=== Creating a disk image from an ISO (i.e. a traditional install method)

The SmartOS/Triton documentation is still around KVM rather than Bhyve. The
process is similar however the `vmadm` documentation is out of date: it
mentions methods and flags that _only_ work with KVM and Bhyve is different.
https://smartos.topicbox.com/groups/smartos-discuss/T1d477bd26c796cad-M6ca9c8317093ee17879656c3[
This is a good thread] that walks the user through the process including the
differing commands.

=== Creating a disk image (_not_ from an ISO)

Example VM image creation from an existing disk image (e.g. lift-and-shift of an
existing VM or an appliance-like image as here. This is based on https://docs.tritondatacenter.com/private-cloud/images/kvm[
the official documentation] with tweaks/fixes accommodating bhyve and a raw
image.

. On the headnode create a working directory in `/var/tmp`:
+
----
[root@headnode (cabin) /var/tmp]# mkdir talos-1.10.6
[root@headnode (cabin) /var/tmp]# cd talos-1.10.6/

----
. Fetch and decompress the image:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# wget https://github.com/siderolabs/talos/releases/download/v1.10.6/metal-amd64.raw.zst
[root@headnode (cabin) /var/tmp/talos-1.10.6]# zstd -d metal-amd64.raw.zst
metal-amd64.raw.zst : 1306525696 bytes
----

. Create an instance definition:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# cat 1.10.6.vmdef.json
{
  "name": "1.10.6",
  "brand": "bhyve",
  "vcpus": 1,
  "autoboot": false,
  "ram": 1024,
  "quota": 20,
  "bootrom": "/usr/share/bhyve/uefi-rom.bin",
  "disks": [
    {
      "boot": true,
      "model": "virtio",
      "size": 10240
    }
  ],
  "nics": [
    {
      "nic_tag": "external",
      "ip": "dhcp",
      "primary": "true",
      "model": "virtio"
    }
  ]
}
----

. Create an empty instance from the definition:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# vmadm create -f talos-1.10.6.vmdef.json
Successfully created VM 8ab900bd-f4a2-4f60-9ccd-e54e51df84b4
----

. Noting the zone ID, copy the disk image to the raw device of the empty instance:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# dd if=metal-amd64.raw of=/dev/zvol/rdsk/zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4/disk0 bs=16M
140+1 records in
140+1 records out
2356150272 bytes (2.2 GiB) transferred in 51.718938 secs (43 MiB/sec)
----
+
TIP: at this point you might boot the instance to make additional changes e.g.
configuration management or installing guest tools etc. (see also cdrom
attachment).

. Retrieve the zvol uuid for the for the instance storage:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# vmadm get 8ab900bd-f4a2-4f60-9ccd-e54e51df84b4 |  json -a disks | json -a zfs_filesystem
zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4/disk0
----

. Create a snapshot of the zvol and copy it to a file (in this example the quota needed to be extended to allow the snapshot creation):
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# zfs get quota zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4
NAME                                        PROPERTY  VALUE  SOURCE
zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4  quota     30.3G  local
[root@headnode (cabin) /var/tmp/talos-1.10.6]# zfs set quota=40G zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4
[root@headnode (cabin) /var/tmp/talos-1.10.6]# zfs snapshot zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4/disk0@dataset
[root@headnode (cabin) /var/tmp/talos-1.10.6]# zfs send zones/8ab900bd-f4a2-4f60-9ccd-e54e51df84b4/disk0@dataset | gzip > talos-1.10.6.zvol.gz
[root@headnode (cabin) /var/tmp/talos-1.10.6]# ls -l talos-1.10.6.zvol.gz
-rw-r--r--   1 root     root     216766946 Aug 10 13:32 talos-1.10.6.zvol.gz
[root@headnode (cabin) /var/tmp/talos-1.10.6]#
----

. Create a IMGAPI manifest (called `talos-1.10.6.manifest.json` that describes
the desired image:
+
----
{
  "v": "2",
  "uuid": "<from the output of uuid>",
  "owner": "<from the output of sdc-ldap s 'login=admin' | grep ^uuid | cut -d' ' -f2>",
  "name": "talos-1.10.6",
  "description": "Talos Linux 1.10.6 (SDC v0.0.1)",
  "version": "0.0.1",
  "state": "active",
  "disabled": false,
  "public": true,
  "os": "linux",
  "type": "zvol",
  "files": [
    {
      "sha1": "<from the output of sum -x sha1 /var/tmp/talos-1.10.6/talos-1.10.6.zvol.gz | cut -d' ' -f1>"
      "size": <from the output of ls -l /var/tmp/talos-1.10.6/talos-1.10.6.zvol.gz | awk '{ print $5 }'>,
      "compression": "gzip"
    }
  ],
  "requirements": {
    "networks": [
      {
        "name": "net0",
        "description": "public"
      }
    ],
   "brand": "bhyve",
   "bootrom": "uefi"
  },
  "image_size": "<as specified in disks.size in talos-1.10.6.vmdef.json>",
  "disk_driver": "virtio",
  "nic_driver": "virtio",
  "cpu_type": "host"
}
----

. Finally, import the image:
+
----
[root@headnode (cabin) /var/tmp/talos-1.10.6]# sdc-imgadm import -m /var/tmp/talos-1.10.6/talos-1.10.6.manifest.json -f /var/tmp/talos-1.10.6/talos-1.10.6.zvol.gz
Imported image 5842ee10-75ef-11f0-9a71-dbc0178393f0 (talos-1.10.6, 0.0.1, state=unactivated)
...75ef-11f0-9a71-dbc0178393f0 [=======================================================>] 100% 206.73MB
Added file "/var/tmp/talos-1.10.6/talos-1.10.6.zvol.gz" (compression "gzip") to image 5842ee10-75ef-11f0-9a71-dbc0178393f0
Activated image 5842ee10-75ef-11f0-9a71-dbc0178393f0
----

The image will now be available for use.

== Running Bhyve instances

=== Console output from bhyve instances

In order to access to the console of a bhyve instance via VNC, the instance must
be running with a UEFI bootrom rather than legacy BIOS. For public images this
is already set so happens automatically. For self-built images one must enable
it in the image https://smartdatacenter.topicbox.com/groups/sdc-discuss/Td4b1c1bb557bae11/spring-2024-hvm-images[thus]:

`sdc-imgadm update <uuid> requirements.brand=bhyve requirements.bootrom=uefi`

For instances not started/managed by Triton, the UEFI bootrom can be enabled on
a stopped instance directly via `vmadm(8)` on the given compute node:

----
vmadm stop <uuid>
vmadm update <uuid> bootrom="/usr/share/bhyve/uefi-rom.bin"
vmadm start <uuid>
----

=== Resizing bhyve instances

Resizing instances cannot be done via the console. There are however three ways
to change the CPU and memory limits for a virtual machine. All require the
instance to be in the stopped state.

. via `vmadm(8)`
. via the Triton VMAPI: `sdc sdc-vmapi /vms/<instance uuid>?action=update -d '{"billing_id":"<new package uuid>"}'`
. via the https://smartdatacenter.topicbox.com/groups/sdc-discuss/T0609521b5cfbff31-Mc983b2dc841bdb365667bf46/resize-a-bhyve-instance[triton CLI]

=== Instance storage resizing

Although the console supports storage resizing, as with resizing instances, it
lags behind the triton CLI. To resize a boot or data disk this can be done on
the given compute node. In the examples below `X` is the linux device, `Y` is
the corresponding ZFS zvol, so `/dev/vda` mapping to `disk0`, `/dev/vdb` to
`disk1` etc.

==== Downsizing

. in the guest unmount the volume and `resize2fs /dev/vdX somesizeM` to shrink
the filesystem.
. shut the instance down
. set the new zvol size on the host compute node: `zfs set volsize=somesize+bufferM zones/<uuid>/diskY`
`somesize+buffer` simply for safety
. boot the instance again and `resize2fs /dev/vdX` to grow the filesystem to the
end of the device

==== Upsizing

. shut the instance down
. set the new zvol size on the host compute node: `zfs set volsize=somesizeM zones/<uuid>/diskY`
. boot the instance
.. if the zvol in question is the boot volume then `cloud-init` will
automatically resize the partition (typically `/dev/vda4`) and extend the
filesystem.
.. if the zvol is a secondary device then depending on the layout `growpart` may
be needed in addition to `resize2fs`


===  Triton packages/PAPI

Feature-wise PAPI is pretty thread-bare. A Triton Package is a bundling of
resources that, in conjunction with an image, define a VM. All salient aspects
of a Package are immutable, and Packages themselves can't be deleted nor
renamed, which is a bit painful. Good news is that in the Console the default
search is for _active_ packages so the churn can be hidden with some fastidious
deactivation.
* quotas in packages being less than the image size will fail silently on
provisioning.

=== Storage

* TODO the two disk thing, must try to find that post
* TODO quotas, reservations and refreservations
* delegated datasets allow some ZFS administrative operations to happen in a
zone. Relevant really only for containers, not HVM.
* quotas can only be set in the global zone
* resizing a zone is as simple (in the global zone) as `zfs set quota=1T zones/9b469b7c-2b46-451b-bcc7-69de7d2f9a1f`
* recordsize can be set in a zone and only takes effect with new files: `zfs set recordsize=1M zones/9b469b7c-2b46-451b-bcc7-69de7d2f9a1f/data`

=== Networked Storage

==== NFS

This is a mix of inexperience wth v4 on my part, the https://docs.smartos.org/configuring-nfs-in-smartos/[
official SmartOS docs being _very_ out of date] (basically, do not use!) and
documentation being increasingly hard to find for things that are not Linux.

You _can_ enable NFS (or SMB) in the global zone but that feels icky if you
think about unix users and other changes needed. Instead I opted to run multiple
zones (small ones too, only 512MB RAM) as that gives much more flexibility:

* changes persist in the zones
* recordsize can be set per share as appropriate for the workload
* NFS share restrictions can also be set as appropriate

General pointers regarding NFS:

* Enabling NFS in a zone (presumably also the global zone, untested...) requires
`svcadm enable rpc/bind` which isn't on by default in Triton SmartOS.
* Enabling a share can be done in a (delegated dataset?) zone thus: `zfs set sharenfs='rw=@172.24.0.254/32' zones/9b469b7c-2b46-451b-bcc7-69de7d2f9a1f/data`
* NFSv4 needs DNS domains to match, not just Unix UIDs: `sharectl set -p nfsmapid_domain=chuci.org nfs`.
Without that, NFS from Linux will work but the mappings will be incorrect:
`user:group` are set correctly server-side but are `nobody:nogroup` client-side.
* In Ubuntu 24.04 (but _not_ 22.04) this also needed to be matched with an
equivalent domain in `/etc/idmapd.conf`.
* NFS performance on spinning disk is particularly poor as NFS will mount `rw`
filesystems `sync` for durability. This can be mitigated with a log special
vdev for the ZIL however the practicalities of that are a subject by themselves.

==== SMB

This is an absolutely massive topic and the documentation is confusing and/or
out of date. My usecase was simple: migration of data from an equivalent share
that had resided on Ubuntu/Samba and had also been simultaneously shared
as NFS described above. No AD, and not even remotely complicated. What follows
is a hodgepodge of https://wiki.smartos.org/configuring-smb-in-smartos/[
the official documentation] (outdated) and https://illumos.topicbox.com/groups/developer/T853ccac866b92198-M029acf623527b9ff13bd3ada[this thread]. I didn't manage to get `sharemgr` to do anything useful
but was able to muddle through with `zfs set sharesmb` once the prerequisites
were done. So steps taken in a zone that was already sharing via NFS (i.e.
unix users matched what existed before/on other instances):

. Edit `pam.conf` to add `pam_smb_passwd.so.1` (see links above, note tabs)
. Enable the services:
+
----
svcadm enable smb/server
svcadm enable smb/client
svcadm enable rpc/bind # already enabled but included for completeness
svcadm enable idmap    # already enabled but included for completeness
----

. Create the unix group
. Create the unix users
. Enable the unix users as SMB users (needed so that their passwords can be
synchronized with `/var/smb/smbpasswd`
+
----
smbadm enable-user <username>
----

. Set the password for the unix user as normal (i.e `passwd <username>`)
. At this point everything wanted to do things with `sharemgr` but I couldn't
get them to work, however `sharesmb` (analogous to `sharenfs`) _did_ work for me:
+
----
zfs set sharesmb=on zones/9b469b7c-2b46-451b-bcc7-69de7d2f9a1f/data
----

=== Migrations

* migrations happen over the admin network, which for me is 1G rather than 10G.


=== CPU caps

By default CPU cap enforcement is enabled and in any production or multi-tenant
environment the recommendation is to leave it on. This is to prevent perceived
scheduling issues for different tenants. Capped and non-capped workloads should
never be mixed as this can cause difficulties for the scheduling of VMAPI/CNAPI.
If both kinds of workloads need to exist a mitigation is with the use of https://docs.tritondatacenter.com/private-cloud/traits[traits]. This https://smartdatacenter.topicbox.com/groups/sdc-discuss/Tdee50d0ae7379e1d[conversation on the rationale, history and issues] is very useful

==== Fixing provisioning errors around "no compute resources"

A single provisioning error can cascade into a DC-wide problem. In one instance
I had a VM migration that went wrong and left a deleted VM still existing in
VMAPI even though it was long gone from its host compute node. The VM object
looked like this:
----
{
  "uuid": "4fe6dceb-37a4-4e18-983c-2230d1e4b802",
  "alias": null,
  "autoboot": null,
  "brand": null,
  "billing_id": null,
  "cpu_cap": null,
  "cpu_shares": null,
  "create_timestamp": null,
  "customer_metadata": {},
  "datasets": [],
  "destroyed": null,
  "firewall_enabled": false,
  "internal_metadata": {},
  "last_modified": null,
  "limit_priv": null,
  "max_locked_memory": null,
  "max_lwps": null,
  "max_physical_memory": null,
  "max_swap": null,
  "nics": [],
  "owner_uuid": null,
  "platform_buildstamp": null,
  "quota": null,
  "ram": null,
  "resolvers": null,
  "server_uuid": "9bb8490c-8aa8-1a29-a45c-d8bbc1cd9188",
  "snapshots": [],
  "state": null,
  "tags": {},
  "zfs_filesystem": null,
  "zfs_io_priority": null,
  "zone_state": null,
  "zonepath": null,
  "zpool": null,
  "image_uuid": null
}
----
amongst other things `cpu_cap: null` stopped CNAPI/VMAPI from automatically
choosing that compute node for new VM replacement, even for a miniscule 64MB
Joyent branded zone, because it would mean mixing capped and uncapped workloads.
The fix was to modify the object and setting the CPU cap via `sdc-vmapi`
 https://github.com/TritonDataCenter/sdc-vmapi/blob/master/docs/index.md#putvm-put-vmsuuid[as per the VMAPI documentation]:

----
sdc-vmapi /vms/4fe6dceb-37a4-4e18-983c-2230d1e4b802? -X PUT -d '{"cpu_cap": "100"}'
HTTP/1.1 200 OK
Connection: close
Content-Type: application/json
Content-Length: 73
Date: Fri, 03 Jan 2025 15:53:15 GMT
Server: VMAPI/9.16.0
x-request-id: 22cb050e-47f7-4bf8-a789-a7abc4810ca6
x-response-time: 129
x-server-name: 54406d2e-1c7c-45fc-a161-e5083e6a2d58

{
  "uuid": "4fe6dceb-37a4-4e18-983c-2230d1e4b802",
  "cpu_cap": "100",
  "tags": {}
}
----

With the cap back in place automatic allocation of new workloads was unblocked.

=== Protecting virtual machines

The console offers no guardrails for deleting HVMs (nor indeed compute nodes)
but their datasets can be protected with: `vmadm update <uuid> indestructible_zoneroot=true`
which will at least stop the zone (therefore, the data) being destroyed.


== Networking

=== Guest agent

Instances that do not have the guest agent cannot be assigned IP addresses
from the Triton DHCP server. In order to get an assignment from an external DHCP
server _Allow DHCP Spoofing_ must be enabled in the console which corresponds to
the `nics["whichever"].allow_dhcp_spoofing` boolean in VMAPI. Unfortunately this
is wasteful in the `external` network address space as an address will be
assigned there but not used.

==== MTU, NICTags and networks

* As stated in the installation documentation, the underlay network MTU *must*
be 9000
* MTU for other networks can only be set at network creation time, even for
the `admin` or `external` networks. For these I believe that this can be
remedied by creating a new network with the desired MTU and then changing the
tags on the NICs. Unclear if this will work for the `admin` network.

=== Compute Node networking

Compute nodes cannot use RealTek Gigabit Ethernet adapters for their admin nic.
For some reason (age?) `dladm` is unable to set the MTU on this driver even
though `show-linkprop rge0` said that the MTU property was read-write. This had
the side effect of a cascading failure for that node resulting in very odd
behaviour from `vmadm`. https://smartdatacenter.topicbox.com/groups/sdc-discuss/Td611bcbb977e00d1[
This is the Triton Topicbox thread].

== Miscellanea

=== ToDo

* headnode console session history
* CNS
* Certificates (needs CNS) (https://github.com/TritonDataCenter/triton-dehydrated[see also this]
* Traits/locality

=== Prometheus and Grafana (this doesn't work)

Prometheus source is https://github.com/TritonDataCenter/triton-prometheus[here]
. The instructions are good but https://github.com/TritonDataCenter/triton-prometheus/blob/master/setup-prometheus.sh[
there is a shell script to crib some needed settings] e.g.

* `sdc-useradm replace-attr admin triton_cns_enabled true`
* `sdc-login -l cns "svcadm restart cns-updater`

and an additional shell script documenting how to install a 16.04 LX zone https://github.com/TritonDataCenter/triton-prometheus/blob/master/setup-prometheus-lx.sh[
is here].

=== Temperatures

Temperaturs are exposed via the FMA and can be read with `fmtopo` thus (8 core,
16 thread CPU so 16 temperatures):

----
[root@cn0 (cabin) ~]# /usr/lib/fm/fmd/fmtopo -V *sensor=temp | grep reading
    reading           double    37.625000
    reading           double    37.625000
    reading           double    37.625000
    reading           double    37.625000
    reading           double    37.625000
    reading           double    37.625000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
    reading           double    37.750000
----

=== Disks

==== Reporting and Failures

The FMA is S.M.A.R.T aware but doesn't expose the complete suite of reporting
that `smartmontools` does, which is not unexpected given that the reporting for
drives from different manufactureres all differ. `smartmontools` is available to
the headnode via `pkgsrc` but compute nodes can't install packages. `smartmontools`
may yet make it into the default SmartOS image but in event that this doesn't
happen, I should see how the PXE images are built, or, as a worst-case
`sdc-oneachnode` can drop a binary. All of a sudden this gets additional
orchestration/configuration management-ey and feels a bit wrong and needs more
research.

==== recordsize, volblocksize, compression and write amplification

By default, the `zones` zpool recordsize is 128k compression is off. For HVM
workloads, when VMAPI creates a virtual machine the volblocksize is by default
8k for the disks and is not configurable currently. Received wisdom is that
enabling `lz4` (`zstd` is not yet available in Illumos ZFS) but this should be
done at CN setup time and my (current) understanding is that HVM instances would
need to be recreated as in a zone migration the zvols properties are going to
be retained.

==== zpool disk selection

https://github.com/TritonDataCenter/smartos-live/blob/master/src/node_modules/disklayout.js[
This is the logic] that the installer uses when provisioning a new node. It may
make choices that are slightly odd (for example on a host with SSD and NVME
pairs it will chose to allocate the NVME as `SLOG` which is not necessarily what
you might want). For more involved layouts you may want to start with base
arrangement at install time then add drives once the node has been provisioned.

=== Migrating or recovering physical nodes

* node UUID is based on https://smartdatacenter.topicbox.com/groups/sdc-discuss/Td611bcbb977e00d1-Mb9105949ffe023f1a0fe82d1/reprovisioning-a-cn-network-early-admin-service-failure[
on the serial number]. No matter how many reinstalls the a node with a given
motherboard will get the same UUID.
* steps for https://docs.tritondatacenter.com/private-cloud/troubleshooting/compute-node[
recovering a Compute Node are relatively straight forward].
* steps for gracefully migrating a headnode, albeit old, https://github.com/TritonDataCenter/triton/pull/172[
are in this pull request].
* steps for recovering a headnode involve physically moving the headnode disks,
as many NICs as is feasible and USB stick. SmartOS will start but Triton likely
will not because NIC labels (especially `admin`) may be wrong; this can be
corrected by mounting the USB stick (`sdc-usbkey mount`) and editing the NIC
tags in `/mnt/usbkey/config`. Following a reboot Triton should start but NICs in
NAPI will still need to be cleaned up, particularly if reusing headnode
hardware.
** `sdc-napi /nics | less` to list
** `curl -X PUT napi.{sdc domain}/nics/{mac} -d belongs_to_uuid=1002ff03-e004-0105-8e06-ad0700080009`
where the ownership is wrong (e.g. old headnode)
** `curl -X DELETE napi.{sdc domain}/nics/{mac}` where the NIC no longer exists

=== Debugging compute node installation issues

Once a CN has been successfully booted, even if it hasn't been adopted it can
still be interrogated with `sdc-oneachnode`.

=== Triton command-line client and CloudAPI

https://www.npmjs.com/package/triton[triton] is a nodejs package and is simple
enough to install (`sudo npm install -g triton`) but the configuration is a
little odd with self-signed certificates requiring the `-i` option and even
though documented, `TRITON_TLS_INSECURE=1` and `SDC_TLS_INSECURE=1` do not work.

The setup will walk through a profile setup requiring the CloudAPI URL, Triton
username and the fingerprint of the SSH key for that user.

TIP: users (including operators it seems) can only see their own instances so,
for example, the admin user can only see SDC components.
